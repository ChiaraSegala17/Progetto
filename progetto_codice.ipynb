{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opzioni per yt-dlp\n",
    "ydl_opts = {\n",
    "    'quiet': True,  \n",
    "    'ignoreerrors': True,  \n",
    "    'extract_flat': False,  \n",
    "    'skip_download': True,  \n",
    "}\n",
    "\n",
    "# Funzione per estrarre URL di video validi\n",
    "def ottieni_video_ita_auto(playlist_url):\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        risultato = ydl.extract_info(playlist_url, download=False)\n",
    "        video_validi = []\n",
    "        if 'entries' in risultato:\n",
    "            for entry in risultato['entries']:\n",
    "                if entry is None:\n",
    "                    continue\n",
    "                controllo_data = entry.get('upload_date')\n",
    "                if controllo_data and int(controllo_data[:4]) >= 2023 and entry.get('duration', 0) >= 900:\n",
    "                    # Verifica sottotitoli forniti dall'autore e automatici in italiano\n",
    "                    sottotitoli_ita = entry.get('subtitles', {}).get('it')\n",
    "                    sottotitoli_ita_auto = entry.get('automatic_captions', {}).get('it')\n",
    "                    if sottotitoli_ita and sottotitoli_ita_auto:  \n",
    "                        video_url = entry.get('webpage_url')\n",
    "                        if video_url:\n",
    "                            video_validi.append(video_url)\n",
    "        print(f\"Video validi trovati: {len(video_validi)}\")\n",
    "        return video_validi\n",
    "\n",
    "# URL della playlist\n",
    "playlist_url = \"https://www.youtube.com/playlist?list=PLBG7i6yKSeBxImaXXoJCK-Zi4sGSlHUrs\"  \n",
    "\n",
    "# Estrazione di URL validi\n",
    "video_urls = ottieni_video_ita_auto(playlist_url)\n",
    "\n",
    "if len(video_urls) >= 15:\n",
    "    video_selezionati = random.sample(video_urls, 15)\n",
    "    print(\"Elenco dei 15 video selezionati:\")\n",
    "    for url in video_selezionati:\n",
    "        print(url)\n",
    "else:\n",
    "    print(f\"Non ci sono abbastanza video validi. Video trovati: {len(video_urls)}.\")\n",
    "    print(\"Elenco dei video disponibili:\")\n",
    "    for url in video_urls:\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sottotitoli forniti dall'autore per i video selezionati\n",
    "\n",
    "cartella_sottotitoli_autore = './sottotitoli_autore'\n",
    "os.makedirs(cartella_sottotitoli_autore, exist_ok=True)\n",
    "\n",
    "\n",
    "ydl_opzioni = {\n",
    "    'ffmpeg_location': r\"C:\\Users\\Silvia\\Desktop\\ffmpeg-2024-10-24-git-153a6dc8fa-essentials_build\\bin\",  # da sostituire con il proprio percorso di ffmpeg\n",
    "    'quiet': True,\n",
    "    'ignoreerrors': True,\n",
    "    'writesubtitles': True,      \n",
    "    'subtitleslangs': ['it'],     \n",
    "    'subtitlesformat': 'srt',     \n",
    "    'skip_download': True,\n",
    "    'outtmpl': os.path.join(cartella_sottotitoli_autore, '%(title)s [%(language)s].srt')\n",
    "}\n",
    "\n",
    "\n",
    "if len(video_selezionati) > 0:\n",
    "    with yt_dlp.YoutubeDL(ydl_opzioni) as ydl:\n",
    "        ydl.download(video_selezionati)\n",
    "else:\n",
    "    print(\"Nessun video disponibile con i criteri specificati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per convertire i file .vtt in .srt nella cartella specificata, ignorando .ipynb_checkpoints\n",
    "# Eliminazione file .vtt originale dopo la conversione\n",
    "\n",
    "def converti_vtt_a_srt(cartella):\n",
    "    for file_name in os.listdir(cartella):\n",
    "        file_path = os.path.join(cartella, file_name)\n",
    "        \n",
    "        \n",
    "        if os.path.isdir(file_path) and file_name == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        \n",
    "        if file_name.endswith('.vtt'):\n",
    "            output_file = file_path.replace('.vtt', '.srt')\n",
    "            \n",
    "            comando = [\n",
    "                'ffmpeg', '-i', file_path,\n",
    "                output_file\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                subprocess.run(comando, check=True)\n",
    "                print(f\"Convertito '{file_name}' in '{output_file}'.\")\n",
    "                \n",
    "                os.remove(file_path)\n",
    "                print(f\"File originale '{file_name}' eliminato.\")\n",
    "                \n",
    "                \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Errore nella conversione di '{file_name}': {e}\")\n",
    "\n",
    "cartella_da_processare = './sottotitoli_autore'\n",
    "\n",
    "converti_vtt_a_srt(cartella_da_processare)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando per trovare ID dei video selezionati\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=6ktrqFoy-e4&t=3s\",\n",
    "    \"https://www.youtube.com/watch?v=BgZoWI6zTjc&t=14s\",\n",
    "    \"https://www.youtube.com/watch?v=QIs9DCzvdIA&t=2s\",\n",
    "    \"https://www.youtube.com/watch?v=aHR1hJBZslI&t=34s\",\n",
    "    \"https://www.youtube.com/watch?v=AQCECeD1Qss\",\n",
    "    \"https://www.youtube.com/watch?v=xMKnDtwLvVY&t=10s\",\n",
    "    \"https://www.youtube.com/watch?v=6q4QJqFpjkk&t=790s\",\n",
    "    \"https://www.youtube.com/watch?v=ZSMDJ-co-Xs\",\n",
    "    \"https://www.youtube.com/watch?v=rX4gGScg2Hs\",\n",
    "    \"https://www.youtube.com/watch?v=Xysy-6pQpsQ&t=45s\",\n",
    "    \"https://www.youtube.com/watch?v=MTBlQJBUhd8\",\n",
    "    \"https://www.youtube.com/watch?v=LWvOlZ4hPU0&t=14s\",\n",
    "    \"https://www.youtube.com/watch?v=GzCB_4JzJIw&t=42s\",\n",
    "    \"https://www.youtube.com/watch?v=opYDgs6eR3c\",\n",
    "    \"https://www.youtube.com/watch?v=jtd_vwY5B8U\"\n",
    "]\n",
    "\n",
    "\n",
    "video_id_pattern = re.compile(r\"v=([a-zA-Z0-9_-]+)\")\n",
    "\n",
    "video_ids = []\n",
    "\n",
    "for url in urls:\n",
    "    match = video_id_pattern.search(url)\n",
    "    if match:\n",
    "        video_ids.append(match.group(1))\n",
    "\n",
    "print(video_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per scaricare i sottotitoli automatici in italiano data una lista di video ID\n",
    "# Prima ottiene la lista delle trascrizioni disponibili e poi tra queste controlla se Ã¨ disponibile la versione automatica in italiano\n",
    "# Scarica la trascrizione e la salva in un file\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import SRTFormatter\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "video_ids = [\n",
    "    '6ktrqFoy-e4', 'BgZoWI6zTjc', 'QIs9DCzvdIA', 'aHR1hJBZslI',\n",
    "    'AQCECeD1Qss', 'xMKnDtwLvVY', '6q4QJqFpjkk', 'ZSMDJ-co-Xs',\n",
    "    'rX4gGScg2Hs', 'Xysy-6pQpsQ', 'MTBlQJBUhd8', 'LWvOlZ4hPU0',\n",
    "    'GzCB_4JzJIw', 'opYDgs6eR3c', 'jtd_vwY5B8U'\n",
    "]\n",
    "\n",
    "cartella_output = 'video_autosub'\n",
    "\n",
    "os.makedirs(cartella_output, exist_ok=True)\n",
    "\n",
    "# Rimuove gli accenti e normalizza il titolo\n",
    "def normalizza_titolo(titolo):\n",
    "    titolo_normalizzato = unicodedata.normalize('NFKD', titolo)\n",
    "    titolo_normalizzato = ''.join([c for c in titolo_normalizzato if not unicodedata.combining(c)])\n",
    "    titolo_normalizzato = re.sub(r'\\W+', '-', titolo_normalizzato.lower())\n",
    "    return titolo_normalizzato.strip('-')\n",
    "\n",
    "def ottieni_titolo(video_id):\n",
    "    try:\n",
    "        with YoutubeDL({'quiet': True}) as ydl:\n",
    "            info = ydl.extract_info(f'https://www.youtube.com/watch?v={video_id}', download=False)\n",
    "            return info.get('title', f'video_{video_id}')\n",
    "    except Exception as e:\n",
    "        print(f\"Impossibile ottenere il titolo per {video_id}: {e}\")\n",
    "        return f'video_{video_id}'\n",
    "\n",
    "def download_sottotitoli_auto(video_id):\n",
    "    try:\n",
    "        trascrizioni = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        \n",
    "        auto_trascrizioni = None\n",
    "        for trascrizione in trascrizioni:\n",
    "            if trascrizione.language_code == 'it' and trascrizione.is_generated:\n",
    "                auto_trascrizioni = trascrizione\n",
    "                break\n",
    "\n",
    "        if auto_trascrizioni:\n",
    "            dati_trascritti = auto_trascrizioni.fetch()\n",
    "            formatter = SRTFormatter()\n",
    "            srt_trascrizione = formatter.format_transcript(dati_trascritti)\n",
    "            if video_id == 'aHR1hJBZslI':\n",
    "                titolo_file = 'i-camionisti-immortali-del-pakistan'\n",
    "            else:\n",
    "                titolo = ottieni_titolo(video_id)\n",
    "                titolo_file = normalizza_titolo(titolo)\n",
    "\n",
    "            # Limita la lunghezza del nome del file a 45 caratteri\n",
    "            # perchÃ¨ file con nomi troppo lunghi potrebbero creare problematiche in punti seguenti del codice\n",
    "            titolo_file = titolo_file[:45]\n",
    "\n",
    "            output_path = os.path.join(cartella_output, f'{titolo_file}.srt')\n",
    "\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(srt_trascrizione)\n",
    "\n",
    "            print(f'Sottotitoli automatici per {video_id} scaricati con successo in italiano e salvati in {cartella_output}.')\n",
    "        else:\n",
    "            print(f'Nessun sottotitolo automatico disponibile in italiano per {video_id}.')\n",
    "    except Exception as e:\n",
    "        print(f'Impossibile scaricare i sottotitoli automatici per {video_id}: {e}')\n",
    "\n",
    "for video_id in video_ids:\n",
    "    download_sottotitoli_auto(video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che pulisce i file scaricati con sottotitoli forniti dall'autore\n",
    "# Regex per identificare e rimuovere le linee con timestamp e testo tra parentesi quadre\n",
    "# Sovrascrittura del file originale con testo pulito\n",
    "\n",
    "def pulisci_sottotitoli(cartella_input):\n",
    "    \n",
    "    timestamp_pattern = re.compile(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}')\n",
    "    parentesi_quadre_pattern = re.compile(r'\\[.*?\\]')\n",
    "    \n",
    "    for file_name in os.listdir(cartella_input):\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "\n",
    "        if file_name.endswith('.srt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            testo_pulito = []\n",
    "            for line in lines:\n",
    "                if not line.strip().isdigit() and not timestamp_pattern.match(line):  \n",
    "                    line = parentesi_quadre_pattern.sub('', line)  \n",
    "                    testo_pulito.append(line.strip())      \n",
    "\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                 file.write(\"\\n\".join(testo_pulito))\n",
    "                \n",
    "            print(f\"Minutaggio e parentesi quadre rimossi dal file: {file_name}\")\n",
    "\n",
    "cartella_da_processare = 'sottotitoli_autore'\n",
    "\n",
    "pulisci_sottotitoli(cartella_da_processare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che pulisce i file scaricati con sottotitoli generati automaticamente da YouTube\n",
    "\n",
    "input_folder = 'video_autosub'\n",
    "output_folder = 'video_autosub_clean'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "timestamp_pattern = re.compile(r\"^\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}$\")\n",
    "non_text_pattern = re.compile(r\"^\\[.*\\]$\") \n",
    "\n",
    "def pulizia_sottotitoli_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        linee = f.readlines()\n",
    "\n",
    "    contenuto_pulito = []\n",
    "    ultima_linea_vuota = True\n",
    "\n",
    "    for linea in linee:\n",
    "        pulizia_linea = linea.strip()\n",
    "\n",
    "        if timestamp_pattern.match(pulizia_linea) or pulizia_linea.isdigit() or non_text_pattern.match(pulizia_linea):\n",
    "            continue\n",
    "        \n",
    "        if pulizia_linea:\n",
    "            contenuto_pulito.append(linea)\n",
    "            ultima_linea_vuota = False\n",
    "        \n",
    "        elif not ultima_linea_vuota:\n",
    "            contenuto_pulito.append('\\n')\n",
    "            ultima_linea_vuota = True\n",
    "\n",
    "    return \"\".join(contenuto_pulito)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.srt'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        contenuto_pulito = pulizia_sottotitoli_file(input_path)\n",
    "\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(contenuto_pulito)\n",
    "\n",
    "        print(f'File processato e salvato: {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per rimuovere la punteggiatura da una parola\n",
    "\n",
    "def pulisci_parole(parola):\n",
    "    return parola.strip(string.punctuation)\n",
    "\n",
    "# Funzione per analizzare le statistiche di un singolo file\n",
    "def ottieni_statistiche_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            text = infile.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la lettura del file {file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    parole = [pulisci_parole(parola) for parola in text.split() if pulisci_parole(parola)]\n",
    "    n_parole = len(parole)\n",
    "    n_uniche = len(set(parole))\n",
    "        \n",
    "    print(f\"\\nAnalisi del file: {file_path}\")\n",
    "    print(\"Numero totale di parole:\", n_parole)\n",
    "    print(\"Numero di parole uniche:\", n_uniche)\n",
    "    print(\"TTR (Type-Token Ratio):\", n_uniche / n_parole)\n",
    "    \n",
    "    counter = Counter(parole)\n",
    "    most_common_words = counter.most_common(10)\n",
    "    \n",
    "    print(\"Frequenze delle parole piÃ¹ comuni:\")\n",
    "    for parola, frequency in most_common_words:\n",
    "        print(\"\\t\", parola, \"(\" + str(frequency) + \")\")\n",
    "\n",
    "# Funzione per iterare su tutti i file .srt nella cartella specificata\n",
    "def analizza_statistiche_cartella (folder_path):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"{folder_path} non Ã¨ una cartella valida.\")\n",
    "        return\n",
    "\n",
    "    if not any(filename.endswith('.srt') for filename in os.listdir(folder_path)):\n",
    "        print(f\"La cartella {folder_path} non contiene file .srt.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and filename.endswith('.srt'):\n",
    "            ottieni_statistiche_file(file_path) \n",
    "\n",
    "analizza_statistiche_cartella(r'sottotitoli_autore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analizza_statistiche_cartella(r'video_autosub_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26.0 scipy==1.10.1 nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni per svolgere la tokenizzazione correttamente\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Funzione per rimuovere caratteri di controllo invisibili\n",
    "def rimuovi_controlli(token):\n",
    "    return ''.join(c for c in token if unicodedata.category(c)[0] != 'C')\n",
    "\n",
    "# Funzione per unire l'apostrofo al token precedente e separare i caratteri successivi\n",
    "def separa_apostrofi(tokens):\n",
    "    nuovi_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == \"McDonald\" and i + 1 < len(tokens) and tokens[i + 1] == \"'s\":\n",
    "            nuovi_tokens.append(\"McDonald's\")\n",
    "            i += 2\n",
    "        elif \"'\" in tokens[i] and tokens[i] != \"'\":\n",
    "            parti = tokens[i].split(\"'\")\n",
    "            if len(parti) == 2 and parti[0] and parti[1]:\n",
    "                nuovi_tokens.append(parti[0] + \"'\")\n",
    "                nuovi_tokens.append(parti[1])\n",
    "            else:\n",
    "                nuovi_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "        elif tokens[i] == \"'\" and i > 0:\n",
    "            nuovi_tokens[-1] += \"'\"\n",
    "            i += 1\n",
    "        else:\n",
    "            nuovi_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return nuovi_tokens\n",
    "\n",
    "# Funzione per separare i trattini iniziali come token singoli\n",
    "def separa_trattini(tokens):\n",
    "    nuovi_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith('-') and len(token) > 1 and not re.match(r'\\b\\w+-\\w+\\b', token):\n",
    "            nuovi_tokens.append('-')\n",
    "            nuovi_tokens.append(token[1:])\n",
    "        else:\n",
    "            nuovi_tokens.append(token)\n",
    "    return nuovi_tokens\n",
    "\n",
    "# Funzione per unire i token contenenti solo 'â' al token precedente\n",
    "def unisci_accento(tokens):\n",
    "    nuovi_tokens = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"â\" and i > 0:  \n",
    "            nuovi_tokens[-1] += token  \n",
    "        else:\n",
    "            nuovi_tokens.append(token)\n",
    "    return nuovi_tokens\n",
    "\n",
    "# Funzione per leggere e tokenizzare i file di sottotitoli\n",
    "def tokenizza_sottotitoli(cartella_input, cartella_output):\n",
    "    if not os.path.exists(cartella_input):\n",
    "        print(\"La cartella di input specificata non esiste.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"La cartella di input esiste e il codice continua...\")\n",
    "    \n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "    files_nella_cartella = sorted(os.listdir(cartella_input))\n",
    "    \n",
    "    for file_name in files_nella_cartella:\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "        \n",
    "        if file_name == '.ipynb_checkpoints':\n",
    "            print(f\"Ignorando la cartella: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.srt'):\n",
    "            print(f\"Elaborando il file: {file_name}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    testo = file.read()\n",
    "                \n",
    "                # Passaggi di tokenizzazione e trasformazione\n",
    "                tokens = word_tokenize(testo)\n",
    "                tokens_apostrofati = separa_apostrofi(tokens)\n",
    "                tokens_trattini_separati = separa_trattini(tokens_apostrofati)\n",
    "                tokens_accento_unito = unisci_accento(tokens_trattini_separati)\n",
    "                tokens_formattati = [f\"[{rimuovi_controlli(token)}]\" for token in tokens_accento_unito]\n",
    "                testo_tokenizzato = \" \".join(tokens_formattati)\n",
    "                \n",
    "                output_path = os.path.join(cartella_output, file_name.replace('.srt', '_tokenized.txt'))\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(testo_tokenizzato)\n",
    "                \n",
    "                print(f\"File tokenizzato salvato in: {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'aprire il file '{file_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Il file '{file_name}' non Ã¨ un file .srt valido o Ã¨ una cartella.\")\n",
    "\n",
    "cartella_input = 'sottotitoli_autore'\n",
    "cartella_output = 'tokenizza_sottotitoli_autore'\n",
    "tokenizza_sottotitoli(cartella_input, cartella_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per la tokenizzazione che tiene conto di fenomeni quali l'apostrofo, il quale viene sempre aggiunto al token precedente\n",
    "# Nella funzione Ã¨ incluso il caso specifico di McDonald's che va tokenizzato come token unico\n",
    "# I file risultanti sono in formato .txt non piÃ¹ .srt\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def rimuovi_controlli(token):\n",
    "    return ''.join(c for c in token if unicodedata.category(c)[0] != 'C')\n",
    "\n",
    "def separa_apostrofi(tokens):\n",
    "    nuovi_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        \n",
    "        if tokens[i] == \"McDonald\" and i + 1 < len(tokens) and tokens[i + 1] == \"'s\":\n",
    "            nuovi_tokens.append(\"McDonald's\")\n",
    "            i += 2  \n",
    "        \n",
    "        elif \"'\" in tokens[i] and tokens[i] != \"'\":\n",
    "            parti = tokens[i].split(\"'\")\n",
    "            if len(parti) == 2 and parti[0] and parti[1]:  \n",
    "                nuovi_tokens.append(parti[0] + \"'\")\n",
    "                nuovi_tokens.append(parti[1])\n",
    "            else:\n",
    "                nuovi_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "        elif tokens[i] == \"'\" and i > 0:  \n",
    "            nuovi_tokens[-1] += \"'\"  \n",
    "            i += 1\n",
    "        else:\n",
    "            nuovi_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return nuovi_tokens\n",
    "\n",
    "def tokenizza_sottotitoli(cartella_input):\n",
    "    if not os.path.exists(cartella_input):\n",
    "        print(\"La cartella specificata non esiste.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"La cartella esiste e il codice continua...\")\n",
    "    \n",
    "    cartella_output = 'sottotitoli_tokenizzati'\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "    \n",
    "    files_nella_cartella = os.listdir(cartella_input)\n",
    "    \n",
    "    for file_name in files_nella_cartella:\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('.srt'):\n",
    "            print(f\"Elaborando il file: {file_name}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    testo = file.read()\n",
    "                \n",
    "                tokens = word_tokenize(testo)\n",
    "                \n",
    "                tokens_apostrofati = separa_apostrofi(tokens)\n",
    "                \n",
    "                tokens_formattati = [f\"[{rimuovi_controlli(token)}]\" for token in tokens_apostrofati]\n",
    "                \n",
    "                output_path = os.path.join(cartella_output, file_name.replace('.srt', '_tokenized.txt'))\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(' '.join(tokens_formattati))  # Salva i token formattati\n",
    "                \n",
    "                print(f\"File tokenizzato salvato in: {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'aprire il file '{file_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Il file '{file_name}' non Ã¨ un file .srt valido o Ã¨ una cartella.\")\n",
    "\n",
    "tokenizza_sottotitoli('video_autosub_clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy version:\", numpy.__version__)\n",
    "print(\"Stanza version:\", stanza.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che prende in input i file tokenizzati e ne esegue la lemmatizzazione\n",
    "\n",
    "if not os.path.exists(os.path.expanduser('~/.stanza_resources/it')):\n",
    "    stanza.download('it')\n",
    "\n",
    "nlp = stanza.Pipeline('it', processors='tokenize,pos,lemma', tokenize_no_ssplit=True)\n",
    "\n",
    "def lemmatizza_token(cartella_input):\n",
    "    if not os.path.exists(cartella_input):\n",
    "        print(\"La cartella specificata non esiste.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"La cartella esiste e il codice continua...\")\n",
    "        \n",
    "    cartella_output = 'sottotitoli_lemmatizzati_autore'\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(cartella_input):\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('_tokenized.txt'):\n",
    "            print(f\"Elaborando il file: {file_name}\")\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    tokens = file.read().split()\n",
    "\n",
    "                tokens_puliti = [token.strip(\"[]\") for token in tokens]\n",
    "\n",
    "                testo = ' '.join(tokens_puliti)\n",
    "                doc = nlp(testo)\n",
    "\n",
    "                tokens_lemmatizzati = [f\"[{word.lemma}]\" for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "                output_path = os.path.join(cartella_output, file_name.replace('_tokenized.txt', '_lemmatized.txt'))\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(' '.join(tokens_lemmatizzati))\n",
    "                print(f\"File lemmatizzato salvato in: {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'elaborazione del file '{file_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Il file '{file_name}' non Ã¨ un file tokenizzato valido o Ã¨ una cartella.\")\n",
    "\n",
    "\n",
    "lemmatizza_token('tokenizza_sottotitoli_autore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che prende in input i file tokenizzati e ne esegue la lemmatizzazione\n",
    "\n",
    "if not os.path.exists(os.path.expanduser('~/.stanza_resources/it')):\n",
    "    stanza.download('it')\n",
    "\n",
    "nlp = stanza.Pipeline('it', processors='tokenize,pos,lemma', tokenize_no_ssplit=True)\n",
    "\n",
    "def lemmatizza_token(cartella_input):\n",
    "    if not os.path.exists(cartella_input):\n",
    "        print(\"La cartella specificata non esiste.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"La cartella esiste e il codice continua...\")\n",
    "        \n",
    "    cartella_output = 'sottotitoli_lemmatizzati_youtube'\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(cartella_input):\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('_tokenized.txt'):\n",
    "            print(f\"Elaborando il file: {file_name}\")\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    tokens = file.read().split()\n",
    "\n",
    "                tokens_puliti = [token.strip(\"[]\") for token in tokens]\n",
    "\n",
    "                testo = ' '.join(tokens_puliti)\n",
    "                doc = nlp(testo)\n",
    "\n",
    "                tokens_lemmatizzati = [f\"[{word.lemma}]\" for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "                output_path = os.path.join(cartella_output, file_name.replace('_tokenized.txt', '_lemmatized.txt'))\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(' '.join(tokens_lemmatizzati))\n",
    "                print(f\"File lemmatizzato salvato in: {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'elaborazione del file '{file_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Il file '{file_name}' non Ã¨ un file tokenizzato valido o Ã¨ una cartella.\")\n",
    "\n",
    "\n",
    "lemmatizza_token('sottotitoli_tokenizzati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che prende in input i file tokenizzati e ne esegue il PoS tagging\n",
    "# Stanza si basa sul Universal Dependencies (UD) tagset per i tag PoS\n",
    "\n",
    "stanza.download('it') \n",
    "nlp = stanza.Pipeline('it', processors='tokenize,pos', tokenize_no_ssplit=True)\n",
    "\n",
    "def pos_tagging(cartella_input):\n",
    "    if not os.path.exists(cartella_input):\n",
    "        print(\"La cartella specificata non esiste.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"La cartella esiste e il codice continua...\")\n",
    "\n",
    "    cartella_output = 'sottotitoli_pos_autore'\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(cartella_input):\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('_tokenized.txt'):\n",
    "            print(f\"Elaborando il file: {file_name}\")\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    tokens = file.read().split()\n",
    "\n",
    "                tokens_puliti = [token.strip(\"[]\") for token in tokens]\n",
    "\n",
    "                testo = ' '.join(tokens_puliti)\n",
    "                doc = nlp(testo)\n",
    "\n",
    "                tokens_pos_tagged = [f\"[{word.text}/{word.upos}]\" for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "                output_path = os.path.join(cartella_output, file_name.replace('_tokenized.txt', '_pos_tagged.txt'))\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(' '.join(tokens_pos_tagged))\n",
    "                \n",
    "                print(f\"File con PoS tagging salvato in: {output_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'aprire il file '{file_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Il file '{file_name}' non Ã¨ un file tokenizzato valido o Ã¨ una cartella.\")\n",
    "\n",
    "pos_tagging('tokenizza_sottotitoli_autore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che prende in input i file tokenizzati e ne esegue il PoS tagging\n",
    "# Stanza si basa sul Universal Dependencies (UD) tagset per i tag PoS\n",
    "\n",
    "stanza.download('it') \n",
    "nlp = stanza.Pipeline('it', processors='tokenize,pos', tokenize_no_ssplit=True)\n",
    "\n",
    "def pos_tagging(cartella_input):\n",
    "    if not os.path.exists(cartella_input):\n",
    "        print(\"La cartella specificata non esiste.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"La cartella esiste e il codice continua...\")\n",
    "\n",
    "    cartella_output = 'sottotitoli_pos_youtube'\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(cartella_input):\n",
    "        file_path = os.path.join(cartella_input, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_name.endswith('_tokenized.txt'):\n",
    "            print(f\"Elaborando il file: {file_name}\")\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    tokens = file.read().split()\n",
    "\n",
    "                tokens_puliti = [token.strip(\"[]\") for token in tokens]\n",
    "\n",
    "                testo = ' '.join(tokens_puliti)\n",
    "                doc = nlp(testo)\n",
    "\n",
    "                tokens_pos_tagged = [f\"[{word.text}/{word.upos}]\" for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "                output_path = os.path.join(cartella_output, file_name.replace('_tokenized.txt', '_pos_tagged.txt'))\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(' '.join(tokens_pos_tagged))\n",
    "                \n",
    "                print(f\"File con PoS tagging salvato in: {output_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'aprire il file '{file_name}': {e}\")\n",
    "        else:\n",
    "            print(f\"Il file '{file_name}' non Ã¨ un file tokenizzato valido o Ã¨ una cartella.\")\n",
    "\n",
    "pos_tagging('sottotitoli_tokenizzati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIZIO CONFRONTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per verificare i file nelle due cartelle\n",
    "\n",
    "def verifica_file(cartella):\n",
    "    if not os.path.exists(cartella):\n",
    "        print(f\"La cartella '{cartella}' non esiste!\")\n",
    "        return\n",
    "    print(f\"\\nFile nella cartella '{cartella}':\")\n",
    "    for file in os.listdir(cartella):\n",
    "        print(f\"- {file}\")\n",
    "\n",
    "\n",
    "verifica_file(\"tokenizza_sottotitoli_autore\")\n",
    "verifica_file(\"sottotitoli_tokenizzati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import hashlib\n",
    "from nltk import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv nuovo_env\n",
    "!nuovo_env\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.4 scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show numpy\n",
    "!pip show scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sklearn\n",
    "import statsmodels\n",
    "import numba\n",
    "import skimage\n",
    "\n",
    "print(\"gensim:\", gensim.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"statsmodels:\", statsmodels.__version__)\n",
    "print(\"numba:\", numba.__version__)\n",
    "print(\"scikit-image:\", skimage.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "!pip show plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurarsi che le due cartelle abbiano lo stesso numero di file \n",
    "# Confronto tokenizzazione tra cartelle\n",
    "\n",
    "cartella_autore = \"tokenizza_sottotitoli_autore\"\n",
    "cartella_youtube = \"sottotitoli_tokenizzati\"\n",
    "\n",
    "file_autore = sorted(os.listdir(cartella_autore))\n",
    "file_youtube = sorted(os.listdir(cartella_youtube))\n",
    "\n",
    "\n",
    "if len(file_autore) != len(file_youtube):\n",
    "    raise ValueError(\"Le due cartelle contengono un numero diverso di file.\")\n",
    "\n",
    "\n",
    "risultati_confronto = []\n",
    "\n",
    "for file_a, file_y in zip(file_autore, file_youtube):\n",
    "    \n",
    "    path_a = os.path.join(cartella_autore, file_a)\n",
    "    path_y = os.path.join(cartella_youtube, file_y)\n",
    "\n",
    "    with open(path_a, \"r\", encoding=\"utf-8\") as fa, open(path_y, \"r\", encoding=\"utf-8\") as fc:\n",
    "        tokens_a = set(fa.read().split())  \n",
    "        tokens_y = set(fc.read().split())  \n",
    "\n",
    "    # Individuazione delle differenze\n",
    "    solo_in_autore = tokens_a - tokens_y\n",
    "    solo_in_youtube = tokens_y - tokens_a\n",
    "\n",
    "    confronto = {\n",
    "        \"File Autore\": file_a,\n",
    "        \"File YouTube\": file_y,\n",
    "        \"Solo in Autore (N. Token)\": len(solo_in_autore),\n",
    "        \"Solo in YouTube (N. Token)\": len(solo_in_youtube),\n",
    "        \"Token Solo in Autore\": \", \".join(sorted(solo_in_autore)) if solo_in_autore else \"Nessuno\",\n",
    "        \"Token Solo in YouTube\": \", \".join(sorted(solo_in_youtube)) if solo_in_youtube else \"Nessuno\",\n",
    "    }\n",
    "    risultati_confronto.append(confronto)\n",
    "\n",
    "# Creazione DataFrame e uso di Plotly per creazione di una tabella interattiva basata sul DataFrame\n",
    "df_confronto = pd.DataFrame(risultati_confronto)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_confronto.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df_confronto[col].tolist() for col in df_confronto.columns],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "df_confronto.to_csv(\"confronto_tokenizzazione_interattivo.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"La tabella del confronto Ã¨ stata salvata in 'confronto_tokenizzazione_interattivo.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto lemmatizzazione\n",
    "\n",
    "cartella_autore = \"sottotitoli_lemmatizzati_autore\"\n",
    "cartella_youtube = \"sottotitoli_lemmatizzati_youtube\"\n",
    "\n",
    "file_autore = sorted(os.listdir(cartella_autore))\n",
    "file_youtube = sorted(os.listdir(cartella_youtube))\n",
    "\n",
    "\n",
    "risultati_confronto = []\n",
    "\n",
    "for file_a, file_y in zip(file_autore, file_youtube):\n",
    "    \n",
    "    path_a = os.path.join(cartella_autore, file_a)\n",
    "    path_y = os.path.join(cartella_youtube, file_y)\n",
    "\n",
    "\n",
    "    with open(path_a, \"r\", encoding=\"utf-8\") as fa, open(path_y, \"r\", encoding=\"utf-8\") as fy:\n",
    "        lemmi_a = set(lemma.lower() for lemma in fa.read().split())  \n",
    "        lemmi_y = set(lemma.lower() for lemma in fy.read().split())  \n",
    "\n",
    "    solo_in_autore = lemmi_a - lemmi_y\n",
    "    solo_in_youtube = lemmi_y - lemmi_a\n",
    "\n",
    "\n",
    "    confronto = {\n",
    "        \"File Autore\": file_a,\n",
    "        \"File YouTube\": file_y,\n",
    "        \"Solo in Autore (N. Lemmi)\": len(solo_in_autore),\n",
    "        \"Solo in YouTube (N. Lemmi)\": len(solo_in_youtube),\n",
    "        \"Lemmi Solo in Autore\": \", \".join(sorted(solo_in_autore)) if solo_in_autore else \"Nessuno\",\n",
    "        \"Lemmi Solo in YouTube\": \", \".join(sorted(solo_in_youtube)) if solo_in_youtube else \"Nessuno\",\n",
    "    }\n",
    "    risultati_confronto.append(confronto)\n",
    "\n",
    "df_confronto = pd.DataFrame(risultati_confronto)\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_confronto.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df_confronto[col].tolist() for col in df_confronto.columns],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "df_confronto.to_csv(\"confronto_lemmatizzazioni_interattivo.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"La tabella del confronto Ã¨ stata salvata in 'confronto_lemmatizzazioni_interattivo.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto PoS Tagging \n",
    "\n",
    "cartella_autore = \"sottotitoli_pos_autore\"\n",
    "cartella_youtube = \"sottotitoli_pos_youtube\"\n",
    "\n",
    "file_autore = sorted(os.listdir(cartella_autore))\n",
    "file_youtube = sorted(os.listdir(cartella_youtube))\n",
    "\n",
    "\n",
    "risultati_confronto = []\n",
    "\n",
    "for file_a, file_y in zip(file_autore, file_youtube):\n",
    "    path_a = os.path.join(cartella_autore, file_a)\n",
    "    path_y = os.path.join(cartella_youtube, file_y)\n",
    "\n",
    "    with open(path_a, \"r\", encoding=\"utf-8\") as fa, open(path_y, \"r\", encoding=\"utf-8\") as fy:\n",
    "        pos_a = set(pos.lower() for pos in fa.read().split())  \n",
    "        pos_y = set(pos.lower() for pos in fy.read().split())  \n",
    "\n",
    "    solo_in_autore = pos_a - pos_y\n",
    "    solo_in_youtube = pos_y - pos_a\n",
    "\n",
    "    confronto = {\n",
    "        \"File Autore\": file_a,\n",
    "        \"File YouTube\": file_y,\n",
    "        \"Solo in Autore (N. Pos)\": len(solo_in_autore),\n",
    "        \"Solo in YouTube (N. Pos)\": len(solo_in_youtube),\n",
    "        \"Pos Solo in Autore\": \", \".join(sorted(solo_in_autore)) if solo_in_autore else \"Nessuno\",\n",
    "        \"Pos Solo in YouTube\": \", \".join(sorted(solo_in_youtube)) if solo_in_youtube else \"Nessuno\",\n",
    "    }\n",
    "    risultati_confronto.append(confronto)\n",
    "\n",
    "\n",
    "df_confronto = pd.DataFrame(risultati_confronto)\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_confronto.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df_confronto[col].tolist() for col in df_confronto.columns],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "df_confronto.to_csv(\"confronto_pos_interattivo.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"La tabella del confronto Ã¨ stata salvata in 'confronto_pos_interattivo.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare e visualizzare le percentuali di ogni PoS tag per ogni file\n",
    "\n",
    "cartella_pos_autore = \"sottotitoli_pos_autore\"\n",
    "cartella_pos_youtube = \"sottotitoli_pos_youtube\"\n",
    "\n",
    "if not os.path.exists(cartella_pos_autore):\n",
    "    raise FileNotFoundError(\"La cartella 'sottotitoli_pos_autore' non esiste.\")\n",
    "if not os.path.exists(cartella_pos_youtube):\n",
    "    raise FileNotFoundError(\"La cartella 'sottotitoli_pos_youtube' non esiste.\")\n",
    "\n",
    "def calcola_frequenze_pos(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokens_pos = []\n",
    "        for token in f.read().split():\n",
    "            if \"/\" in token:\n",
    "                pos = token.strip(\"[]\").rsplit(\"/\", 1)[-1] \n",
    "                tokens_pos.append(pos)\n",
    "            else:\n",
    "                print(f\"Token non conforme ignorato: {token}\")  \n",
    "        conteggi_pos = Counter(tokens_pos)\n",
    "        totale_token = sum(conteggi_pos.values())\n",
    "        return {pos: (conteggio / totale_token) * 100 for pos, conteggio in conteggi_pos.items()}\n",
    "\n",
    "file_autore = sorted(os.listdir(cartella_pos_autore))\n",
    "file_youtube = sorted(os.listdir(cartella_pos_youtube))\n",
    "\n",
    "\n",
    "risultati = []\n",
    "\n",
    "for file_a, file_y in zip(file_autore, file_youtube):\n",
    "    path_a = os.path.join(cartella_pos_autore, file_a)\n",
    "    path_y = os.path.join(cartella_pos_youtube, file_y)\n",
    "\n",
    "    frequenze_autore = calcola_frequenze_pos(path_a)\n",
    "    frequenze_youtube = calcola_frequenze_pos(path_y)\n",
    "\n",
    "    tutti_pos = set(frequenze_autore.keys()).union(set(frequenze_youtube.keys()))\n",
    "    for pos in tutti_pos:\n",
    "        risultati.append({\n",
    "            \"Nome File Autore\": file_a,\n",
    "            \"Nome File YouTube\": file_y,\n",
    "            \"PoS\": pos,\n",
    "            \"Percentuale Autore\": round(frequenze_autore.get(pos, 0), 2),\n",
    "            \"Percentuale YouTube\": round(frequenze_youtube.get(pos, 0), 2)\n",
    "        })\n",
    "\n",
    "df_frequenze = pd.DataFrame(risultati)\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_frequenze.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df_frequenze[col].tolist() for col in df_frequenze.columns],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per visualizzare la percentuale complessiva dei PoS tag nei file \n",
    "\n",
    "cartella_pos_autore = \"sottotitoli_pos_autore\"\n",
    "cartella_pos_youtube = \"sottotitoli_pos_youtube\"\n",
    "\n",
    "\n",
    "if not os.path.exists(cartella_pos_autore):\n",
    "    raise FileNotFoundError(\"La cartella 'sottotitoli_pos_autore' non esiste.\")\n",
    "if not os.path.exists(cartella_pos_youtube):\n",
    "    raise FileNotFoundError(\"La cartella 'sottotitoli_pos_youtube' non esiste.\")\n",
    "\n",
    "def calcola_frequenze_pos(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokens_pos = []\n",
    "        for token in f.read().split():\n",
    "            if \"/\" in token:\n",
    "                pos = token.strip(\"[]\").rsplit(\"/\", 1)[-1]  \n",
    "                tokens_pos.append(pos)\n",
    "        conteggi_pos = Counter(tokens_pos)\n",
    "        totale_token = sum(conteggi_pos.values())\n",
    "        return {pos: (conteggio / totale_token) * 100 for pos, conteggio in conteggi_pos.items()}\n",
    "\n",
    "file_autore = sorted(os.listdir(cartella_pos_autore))\n",
    "file_youtube = sorted(os.listdir(cartella_pos_youtube))\n",
    "\n",
    "frequenze_totali_autore = Counter()\n",
    "frequenze_totali_youtube = Counter()\n",
    "totale_file = len(file_autore)\n",
    "\n",
    "for file_a, file_y in zip(file_autore, file_youtube):\n",
    "    path_a = os.path.join(cartella_pos_autore, file_a)\n",
    "    path_y = os.path.join(cartella_pos_youtube, file_y)\n",
    "\n",
    "    frequenze_autore = calcola_frequenze_pos(path_a)\n",
    "    frequenze_youtube = calcola_frequenze_pos(path_y)\n",
    "\n",
    "    for pos, perc in frequenze_autore.items():\n",
    "        frequenze_totali_autore[pos] += perc\n",
    "    for pos, perc in frequenze_youtube.items():\n",
    "        frequenze_totali_youtube[pos] += perc\n",
    "\n",
    "percentuali_medie_autore = {pos: round(total / totale_file, 2) for pos, total in frequenze_totali_autore.items()}\n",
    "percentuali_medie_youtube = {pos: round(total / totale_file, 2) for pos, total in frequenze_totali_youtube.items()}\n",
    "\n",
    "tutti_pos = set(percentuali_medie_autore.keys()).union(set(percentuali_medie_youtube.keys()))\n",
    "\n",
    "risultati = []\n",
    "for pos in tutti_pos:\n",
    "    risultati.append({\n",
    "        \"PoS\": pos,\n",
    "        \"Percentuale PoS autore\": percentuali_medie_autore.get(pos, 0),\n",
    "        \"Percentuale PoS youTube\": percentuali_medie_youtube.get(pos, 0)\n",
    "    })\n",
    "\n",
    "df_risultati = pd.DataFrame(risultati)\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_risultati.columns),\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'),\n",
    "    cells=dict(values=[df_risultati[col].tolist() for col in df_risultati.columns],\n",
    "               fill_color='lavender',\n",
    "               align='left'))\n",
    "])\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
